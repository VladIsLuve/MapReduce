# MapReduce
Попытки использовать Hadoop MapReduce API в языке Java (Task1, Task2) и библиотеку PySpark в Python (Task3).

Условия заданий:
Task1 https://docs.google.com/document/d/1zMzeHpah8BjZL2Paif4R7rSzdc0xc1ZPHbbhhdxztVU/edit
Task2 https://docs.google.com/document/d/1d9tJ2fk__0XfnsLi06Pt1jaUyxbkH1WkoWAsq-Fy2AQ/edit
Task3 https://docs.google.com/document/d/1sSHZtEatNhFGaYIQJ9RocMAi_v6pA_qti3WDavx9bk4/edit

Если вкратце, задания 1,3 требуют обработки большого набора данных (привести данные о финансовых сделках к формату японских свечей), задание 2 состояло в умножении матриц большого размера, которые целиком не помещаются в операционной памяти.
